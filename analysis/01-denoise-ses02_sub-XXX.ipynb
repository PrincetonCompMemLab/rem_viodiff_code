{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI Data Loading and Normalization in Python \n",
    "**V.0.2 - Beta, [Contributions](#contributions)**   \n",
    "\n",
    "### Goal of this script\n",
    " 1. load the fMRI data into python\n",
    "     - 1 postfaces run\n",
    "     - 2 familiarization runs\n",
    "     - 4 reward runs\n",
    "     - 2 decision runs\n",
    "     - note that postscenes is part of ses-02, but was processed with the ses-01 runs\n",
    " 2. create an average brain mask from multiple runs\n",
    "     - ses02_brain (10 task runs)\n",
    " 3. trim TRs from the beginning AND end of each run (and apply this trimming to the confounds as well). Number of volumes trimmed defined by n_trunc_beginning and n_trunc_end. \n",
    "     - save volume as _trimXandXTRs.nii.gz\n",
    " 4. apply a high-pass filter and z-score the data\n",
    "     - save volume as _trimXandXTRs_normalized.nii.gz\n",
    " 5. concatenate runs to make one time series for various analyses\n",
    "     - familiarization runs (2)\n",
    "     - reward runs (4)\n",
    " 6. plot a timeseries for a voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 'sub-XXX'\n",
    "ses = 'ses-02'\n",
    "task1='postFaces'\n",
    "task2='familiarization'\n",
    "task3='reward'\n",
    "task4='decision'\n",
    "n_trunc_beginning=9 #Number of volumes to trim from beginning of run\n",
    "n_trunc_end=5 #Number of volumes to trim from end of run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker,  MultiNiftiMasker\n",
    "from nilearn.masking import intersect_masks\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.plotting import plot_anat\n",
    "from nilearn.plotting import plot_epi\n",
    "from nilearn.image.image import mean_img\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "#import seaborn as sns \n",
    "import scipy.io\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5\n",
    "#sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some helper functions\n",
    "sys.path.insert(0, '/jukebox/norman/emcdevitt/studies/SVD/code/mainanalysis')\n",
    "import svd_utils\n",
    "from svd_utils import load_svd_epi_data, load_data\n",
    "\n",
    "# load some constants\n",
    "from svd_utils import svd_dir, svd_bids_dir, svd_TR, svd_hrf_lag, run_names, n_runs, TRs_run\n",
    "\n",
    "print('TASKS:', task1, task2, task3, task4)\n",
    "print('LIST OF ALL TASKS:', run_names)\n",
    "task1_index = run_names.index(task1)\n",
    "task2_index = run_names.index(task2)\n",
    "task3_index = run_names.index(task3)\n",
    "task4_index = run_names.index(task4)\n",
    "print('task index:', task1_index)\n",
    "print('task index:', task2_index)\n",
    "print('task index:', task3_index)\n",
    "print('task index:', task4_index)\n",
    "print('')\n",
    "\n",
    "n_runs_postfaces = n_runs[task1_index]\n",
    "n_runs_familiarization = n_runs[task2_index]\n",
    "n_runs_reward = n_runs[task3_index]\n",
    "n_runs_decision = n_runs[task4_index]\n",
    "n_runs_total = n_runs_postfaces + n_runs_familiarization + n_runs_reward + n_runs_decision\n",
    "\n",
    "TRs_run_postfaces=TRs_run[task1_index]\n",
    "TRs_run_familiarization=TRs_run[task2_index]\n",
    "TRs_run_reward=TRs_run[task3_index]\n",
    "TRs_run_decision=TRs_run[task4_index]\n",
    "\n",
    "anat_dir=svd_bids_dir + 'derivatives/deface/'\n",
    "out_dir= svd_bids_dir + 'derivatives/firstlevel/%s/' % sub\n",
    "mask_fold = svd_bids_dir + 'derivatives/firstlevel/%s/masks/' % sub\n",
    "\n",
    "ses0_dir=svd_bids_dir + 'derivatives/fmriprep/%s/ses-00/func/' % sub\n",
    "ses1_dir=svd_bids_dir + 'derivatives/fmriprep/%s/ses-01/func/' % sub\n",
    "ses2_dir=svd_bids_dir + 'derivatives/fmriprep/%s/ses-02/func/' % sub\n",
    "\n",
    "print('bids dir = %s' % (svd_bids_dir))\n",
    "print('')\n",
    "print('subject dir = %s' % (ses2_dir))\n",
    "print('')\n",
    "print('output dir = %s' % (out_dir))\n",
    "print('')\n",
    "print('number of postfaces runs = %d' % (n_runs_postfaces))\n",
    "print('number of familiarization runs = %d' % (n_runs_familiarization))\n",
    "print('number of reward runs = %d' % (n_runs_reward))\n",
    "print('number of decision runs = %d' % (n_runs_decision))\n",
    "print('TR = %s seconds' % (svd_TR))\n",
    "print('TRs per postfaces run = %s' % (TRs_run_postfaces))\n",
    "print('TRs per familiarization run = %s' % (TRs_run_familiarization))\n",
    "print('TRs per reward run = %s' % (TRs_run_reward))\n",
    "print('TRs per decision run = %s' % (TRs_run_decision))\n",
    "print('trim %d volumes from beginning of each run' % (n_trunc_beginning))\n",
    "print('trim %d volumes from end of each run' % (n_trunc_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select confounds and trim volumes from confounds file\n",
    "Choose the desired confounds from the confounds_regressors.tsv file from fmriprep, trim the columns corresponding to trimmed volumes, and save as a .txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSTFACES\n",
    "task1='postfaces' #update for file naming purposes\n",
    "confounds=[]\n",
    "mc_all=[]\n",
    "print(run_names[task1_index], 'Number of runs:', n_runs_postfaces, 'TRs per run:', TRs_run_postfaces, '-->', TRs_run_postfaces-n_trunc_beginning-n_trunc_end)\n",
    "\n",
    "for r in range(1,n_runs_postfaces+1):\n",
    "    fname='_ses-02_task-%s_run-0%i_desc-confounds_regressors.tsv' % (task1, r)\n",
    "    confounds = pd.read_csv(ses2_dir + sub + fname,  sep='\\t', header=(0))\n",
    "    #confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:] #only trim beginning\n",
    "    confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:-n_trunc_end] #trim beginning and end\n",
    "    confounds_selected=pd.DataFrame(confounds_selected)\n",
    "    confounds_selected.to_csv(out_dir + 'ses-02/' + sub + '_ses-02_task-%s_run-0%i_confounds_selected_trim%dand%dTRs.txt' % (task1, r, n_trunc_beginning, n_trunc_end), index=False, sep='\\t', mode='w')\n",
    "    print(confounds_selected.shape)\n",
    "    \n",
    "# FAMILIARIZATION\n",
    "confounds=[]\n",
    "mc_all=[]\n",
    "print(run_names[task2_index], 'Number of runs:', n_runs_familiarization, 'TRs per run:', TRs_run_familiarization, '-->', TRs_run_familiarization-n_trunc_beginning-n_trunc_end)\n",
    "\n",
    "for r in range(1,n_runs_familiarization+1):\n",
    "    fname='_ses-02_task-%s_run-0%i_desc-confounds_regressors.tsv' % (task2, r)\n",
    "    confounds = pd.read_csv(ses2_dir + sub + fname,  sep='\\t', header=(0))\n",
    "    #confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:] #only trim beginning\n",
    "    confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:-n_trunc_end] #trim beginning and end\n",
    "    confounds_selected=pd.DataFrame(confounds_selected)\n",
    "    confounds_selected.to_csv(out_dir + 'ses-02/' + sub + '_ses-02_task-%s_run-0%i_confounds_selected_trim%dand%dTRs.txt' % (task2, r, n_trunc_beginning, n_trunc_end), index=False, sep='\\t', mode='w')\n",
    "    print(confounds_selected.shape)\n",
    "    \n",
    "# REWARD\n",
    "confounds=[]\n",
    "mc_all=[]\n",
    "print(run_names[task3_index], 'Number of runs:', n_runs_reward, 'TRs per run:', TRs_run_reward, '-->', TRs_run_reward-n_trunc_beginning-n_trunc_end)\n",
    "\n",
    "for r in range(1,n_runs_reward+1):\n",
    "    fname='_ses-02_task-%s_run-0%i_desc-confounds_regressors.tsv' % (task3, r)\n",
    "    confounds = pd.read_csv(ses2_dir + sub + fname,  sep='\\t', header=(0))\n",
    "    #confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:] #only trim beginning\n",
    "    confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:-n_trunc_end] #trim beginning and end\n",
    "    confounds_selected=pd.DataFrame(confounds_selected)\n",
    "    confounds_selected.to_csv(out_dir + 'ses-02/' + sub + '_ses-02_task-%s_run-0%i_confounds_selected_trim%dand%dTRs.txt' % (task3, r, n_trunc_beginning, n_trunc_end), index=False, sep='\\t', mode='w')\n",
    "    print(confounds_selected.shape)\n",
    "    \n",
    "# DECISION\n",
    "confounds=[]\n",
    "mc_all=[]\n",
    "print(run_names[task4_index], 'Number of runs:', n_runs_decision, 'TRs per run:', TRs_run_decision, '-->', TRs_run_decision-n_trunc_beginning-n_trunc_end)\n",
    "\n",
    "for r in range(1,n_runs_decision+1):\n",
    "    fname='_ses-02_task-%s_run-0%i_desc-confounds_regressors.tsv' % (task4, r)\n",
    "    confounds = pd.read_csv(ses2_dir + sub + fname,  sep='\\t', header=(0))\n",
    "    #confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:] #only trim beginning\n",
    "    confounds_selected=confounds[['trans_x','trans_y','trans_z','rot_x','rot_y','rot_z']][n_trunc_beginning:-n_trunc_end] #trim beginning and end\n",
    "    confounds_selected=pd.DataFrame(confounds_selected)\n",
    "    confounds_selected.to_csv(out_dir + 'ses-02/' + sub + '_ses-02_task-%s_run-0%i_confounds_selected_trim%dand%dTRs.txt' % (task4, r, n_trunc_beginning, n_trunc_end), index=False, sep='\\t', mode='w')\n",
    "    print(confounds_selected.shape)\n",
    "    \n",
    "print ('Confound files completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an average mask\n",
    "\n",
    "Make an average mask by intersecting the mask for each run (include postscenes run in ses-02 average brain mask). Plot average mask overlayed on subject's defaced T1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_imgs=[]\n",
    "for run in range(1,n_runs_total+2):\n",
    "    if run==5:\n",
    "        real_run=1\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-postscenes_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    elif run==6:\n",
    "        real_run=1\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-postfaces_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    elif run==7:\n",
    "        real_run=1\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-familiarization_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    elif run==8:\n",
    "        real_run=2\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-familiarization_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    elif run==9:\n",
    "        real_run=1\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-decision_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    elif run==10:\n",
    "        real_run=2\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-decision_run-0%i_space-T1w_desc-brain_mask.nii.gz' % real_run\n",
    "    else:\n",
    "        mask_name = ses2_dir + sub + '_ses-02_task-reward_run-0%i_space-T1w_desc-brain_mask.nii.gz' % run\n",
    "\n",
    "    mask_imgs.append(mask_name)\n",
    "\n",
    "# intersect brain masks\n",
    "avg_mask=intersect_masks(mask_imgs, threshold=0.5, connected=True)\n",
    "\n",
    "# plot\n",
    "t1_file = anat_dir + sub + '_desc-preproc_T1w_defaced.nii.gz'\n",
    "t1_img = image.load_img(t1_file)\n",
    "plot_roi(avg_mask, bg_img=t1_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mask\n",
    "output_name = mask_fold + '%s_%s_brain.nii.gz' % (sub, ses)\n",
    "print('Save average mask:', output_name)\n",
    "print('')\n",
    "\n",
    "dimsize=avg_mask.header.get_zooms()\n",
    "affine_mat = avg_mask.affine\n",
    "print('Mask dimensions:', dimsize)\n",
    "print('')\n",
    "print('Affine:')\n",
    "print(affine_mat)\n",
    "\n",
    "hdr = avg_mask.header  # get a handle for the .nii file's header\n",
    "hdr.set_zooms((dimsize[0], dimsize[1], dimsize[2]))\n",
    "nib.save(avg_mask, output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop first few TRs; apply mask and normalize data\n",
    "\n",
    "`n_trunc_beginning` and `n_trunc_end` sets the number of TRs to drop from beginning and end of volume, respectively.\n",
    "\n",
    "#### Get voxels from an ROI\n",
    "\n",
    "We will extract BOLD data, only for voxels in a mask, by executing the following sequence of steps: \n",
    "1. load whole brain fMRI data (for a given subject and a given run)\n",
    "2. trim TRs from beginning and end if desired\n",
    "2. load the desired mask\n",
    "3. use `NiftiMasker` to sub-select mask voxels from the whole brain data, apply a high-pass filter and normalize\n",
    "    - `NiftiMasker` is a function from nilearn. Here's <a href=\"https://nilearn.github.io/auto_examples/04_manipulating_images/plot_mask_computation.html\">an example</a> about how to use it, and here's the official <a href=\"https://nilearn.github.io/modules/generated/nilearn.input_data.NiftiMasker.html\">documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epi_mask_data_familiarization=[]\n",
    "epi_mask_data_reward=[]\n",
    "epi_mask_data_decision=[]\n",
    "\n",
    "for run in range(1,n_runs_total+1):\n",
    "    print('now on run:', run)\n",
    "    epi_trunc=[]\n",
    "    \n",
    "    # define NiftiMasker function\n",
    "    epi_masker= NiftiMasker(\n",
    "        mask_img=avg_mask,  \n",
    "        high_pass=1/128,\n",
    "        standardize=True,  # Are you going to zscore the data across time?\n",
    "        t_r=svd_TR, \n",
    "        memory='nilearn_cache',  # Caches the mask in the directory given as a string here so that it is easier to load and retrieve\n",
    "        memory_level=1,  # How much memory will you cache?\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # load BOLD data\n",
    "    if run==5:\n",
    "        real_run=1\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-postfaces_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % real_run\n",
    "    elif run==6:\n",
    "        real_run=1\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-familiarization_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % real_run\n",
    "    elif run==7:\n",
    "        real_run=2\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-familiarization_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % real_run\n",
    "    elif run==8:\n",
    "        real_run=1\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-decision_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % real_run\n",
    "    elif run==9:\n",
    "        real_run=2\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-decision_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % real_run\n",
    "    else:\n",
    "        epi_file = ses2_dir + sub + '_ses-02_task-reward_run-0%i_space-T1w_desc-preproc_bold.nii.gz' % run\n",
    "\n",
    "    epi_data=nib.load(epi_file)\n",
    "    epi=epi_data.get_fdata()\n",
    "    \n",
    "    # trim beginning and end; call this epi_trunc\n",
    "    epi_trunc =np.zeros((epi_data.shape[0], epi_data.shape[1], epi_data.shape[2], epi_data.shape[3]-n_trunc_beginning-n_trunc_end))\n",
    "    epi_trunc[:, :, :, :] = epi[:,:,:,n_trunc_beginning:-n_trunc_end]\n",
    "    \n",
    "    # epi_trunc\n",
    "    print('input filename: %s' % (epi_file))\n",
    "    print('')\n",
    "    print('Original:', epi_data.shape, '  ', 'Truncated:', epi_trunc.shape)\n",
    "    dimsize=epi_data.header.get_zooms()\n",
    "    print('Dimensions:', dimsize)\n",
    "    orig_dimsize=dimsize\n",
    "    affine_mat = epi_data.affine  #What is the orientation of the data?\n",
    "    print('Affine:')\n",
    "    print(affine_mat)\n",
    "    print('')\n",
    "    \n",
    "    epi_trunc_nii = nib.Nifti1Image(epi_trunc, affine_mat) #convert to nifti\n",
    "    \n",
    "    # If we were saving the data from this intermediate step (trimmed but not normalized)\n",
    "    #hdr = epi_trunc_nii.header  # get a handle for the .nii file's header\n",
    "    #hdr.set_zooms((dimsize[0], dimsize[1], dimsize[2], dimsize[3]))\n",
    "    #nib.save(epi_trunc_nii, output_name)\n",
    "    \n",
    "    # apply epi_masker function to truncated data\n",
    "    epi_mask_data = epi_masker.fit_transform(epi_trunc_nii)\n",
    "    \n",
    "    # make concatenated data structures\n",
    "    if run==1:\n",
    "        epi_mask_data_reward=epi_mask_data\n",
    "    elif run==2:\n",
    "        epi_mask_data_reward=np.vstack([epi_mask_data_reward,epi_mask_data])\n",
    "    elif run==3:\n",
    "        epi_mask_data_reward=np.vstack([epi_mask_data_reward,epi_mask_data])\n",
    "    elif run==4:\n",
    "        epi_mask_data_reward=np.vstack([epi_mask_data_reward,epi_mask_data])\n",
    "    elif run==6:\n",
    "        epi_mask_data_familiarization=epi_mask_data\n",
    "    elif run==7:\n",
    "        epi_mask_data_familiarization=np.vstack([epi_mask_data_familiarization,epi_mask_data])\n",
    "    elif run==8:\n",
    "        epi_mask_data_decision=epi_mask_data\n",
    "    elif run==9:\n",
    "        epi_mask_data_decision=np.vstack([epi_mask_data_decision,epi_mask_data])\n",
    "    else:\n",
    "        epi_mask_data_postfaces=epi_mask_data\n",
    "    \n",
    "    affine_mat = avg_mask.affine #should be the same as the epi data\n",
    "    avg_mask.shape\n",
    "    coords = np.where(avg_mask.get_fdata())\n",
    "    bold_vol=[]\n",
    "    bold_vol=np.zeros((avg_mask.shape[0], avg_mask.shape[1], avg_mask.shape[2], epi_mask_data.shape[0]))\n",
    "    bold_vol[coords[0], coords[1], coords[2], :] = epi_mask_data.T\n",
    "    print('epi_mask_data shape:', bold_vol.shape)\n",
    "    print('')\n",
    "    \n",
    "    if run==5:\n",
    "        real_run=1\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-postfaces_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, real_run, n_trunc_beginning, n_trunc_end))\n",
    "    elif run==6:\n",
    "        real_run=1\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-familiarization_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, real_run, n_trunc_beginning, n_trunc_end))\n",
    "    elif run==7:\n",
    "        real_run=2\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-familiarization_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, real_run, n_trunc_beginning, n_trunc_end))\n",
    "    elif run==8:\n",
    "        real_run=1\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-decision_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, real_run, n_trunc_beginning, n_trunc_end))\n",
    "    elif run==9:\n",
    "        real_run=2\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-decision_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, real_run, n_trunc_beginning, n_trunc_end))\n",
    "    else:\n",
    "        output_name = (out_dir + 'ses-02/' + '%s_ses-02_task-reward_run-0%i_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, run, n_trunc_beginning, n_trunc_end))\n",
    "    \n",
    "    # Save the truncated, normalized data\n",
    "    print('Saving trimmed and normalized volume for run',run, output_name)\n",
    "    print('output filename:', output_name)\n",
    "    bold_nii = nib.Nifti1Image(bold_vol, affine_mat) #convert to nifti\n",
    "    hdr = bold_nii.header  #get a handle for the .nii file's header\n",
    "    hdr.set_zooms((orig_dimsize[0], orig_dimsize[1], orig_dimsize[2], orig_dimsize[3]))\n",
    "    nib.save(bold_nii, output_name)\n",
    "        \n",
    "print('Volumes saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and save concatenated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_mask.shape\n",
    "coords = np.where(avg_mask.get_fdata())\n",
    "dimsize=avg_mask.header.get_zooms()\n",
    "print('Voxel dimensions:', dimsize)\n",
    "\n",
    "affine_mat = avg_mask.affine  # What is the orientation of the data\n",
    "\n",
    "# transform concatenated volumes and save\n",
    "\n",
    "# FAMILIARIZATION\n",
    "output_name = out_dir + 'ses-02/' + '%s_ses-02_task-familiarization_run-ALL_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, n_trunc_beginning, n_trunc_end)\n",
    "bold_vol=[]\n",
    "bold_vol =np.zeros((avg_mask.shape[0], avg_mask.shape[1], avg_mask.shape[2], epi_mask_data_familiarization.shape[0]))\n",
    "bold_vol[coords[0], coords[1], coords[2], :] = epi_mask_data_familiarization.T\n",
    "\n",
    "print('concatenate familiarization runs')\n",
    "print('avg_mask shape:', avg_mask.shape)\n",
    "print('epi_mask_data shape:', bold_vol.shape)\n",
    "print('epi_mask_data_familiarization shape(timepoints, voxels):', epi_mask_data_familiarization.shape)\n",
    "print('')\n",
    "print('output filename:', output_name)\n",
    "print('')\n",
    "bold_nii = nib.Nifti1Image(bold_vol, affine_mat)\n",
    "hdr = bold_nii.header  # get a handle for the .nii file's header\n",
    "print('Dimensions:', orig_dimsize) #4th dimension is TR\n",
    "print('')\n",
    "hdr.set_zooms((orig_dimsize[0], orig_dimsize[1], orig_dimsize[2], orig_dimsize[3]))\n",
    "nib.save(bold_nii, output_name)\n",
    "print('Volume saved')\n",
    "\n",
    "# REWARD\n",
    "output_name = out_dir + 'ses-02/' + '%s_ses-02_task-reward_run-ALL_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, n_trunc_beginning, n_trunc_end)\n",
    "bold_vol=[]\n",
    "bold_vol =np.zeros((avg_mask.shape[0], avg_mask.shape[1], avg_mask.shape[2], epi_mask_data_reward.shape[0]))\n",
    "bold_vol[coords[0], coords[1], coords[2], :] = epi_mask_data_reward.T\n",
    "\n",
    "print('concatenate reward runs')\n",
    "print('avg_mask shape:', avg_mask.shape)\n",
    "print('epi_mask_data shape:', bold_vol.shape)\n",
    "print('epi_mask_data_reward shape(timepoints, voxels):', epi_mask_data_reward.shape)\n",
    "print('')\n",
    "print('output filename:', output_name)\n",
    "print('')\n",
    "bold_nii = nib.Nifti1Image(bold_vol, affine_mat)\n",
    "hdr = bold_nii.header  # get a handle for the .nii file's header\n",
    "print('Dimensions:', orig_dimsize) #4th dimension is TR\n",
    "print('')\n",
    "hdr.set_zooms((orig_dimsize[0], orig_dimsize[1], orig_dimsize[2], orig_dimsize[3]))\n",
    "nib.save(bold_nii, output_name)\n",
    "print('Volume saved')\n",
    "\n",
    "# DECISION\n",
    "output_name = out_dir + 'ses-02/' + '%s_ses-02_task-decision_run-ALL_space-T1w_desc-preproc_bold_trim%dand%dTRs_normalized.nii.gz' % (sub, n_trunc_beginning, n_trunc_end)\n",
    "bold_vol=[]\n",
    "bold_vol =np.zeros((avg_mask.shape[0], avg_mask.shape[1], avg_mask.shape[2], epi_mask_data_decision.shape[0]))\n",
    "bold_vol[coords[0], coords[1], coords[2], :] = epi_mask_data_decision.T\n",
    "\n",
    "print('concatenate decision runs')\n",
    "print('avg_mask shape:', avg_mask.shape)\n",
    "print('epi_mask_data shape:', bold_vol.shape)\n",
    "print('epi_mask_data_decision shape(timepoints, voxels):', epi_mask_data_decision.shape)\n",
    "print('')\n",
    "print('output filename:', output_name)\n",
    "print('')\n",
    "bold_nii = nib.Nifti1Image(bold_vol, affine_mat)\n",
    "hdr = bold_nii.header  # get a handle for the .nii file's header\n",
    "print('Dimensions:', orig_dimsize) #4th dimension is TR\n",
    "print('')\n",
    "hdr.set_zooms((orig_dimsize[0], orig_dimsize[1], orig_dimsize[2], orig_dimsize[3]))\n",
    "nib.save(bold_nii, output_name)\n",
    "print('Volume saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a volume from each run to check normalization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol_num=0\n",
    "# for run in range(1,svd_n_runs+1):\n",
    "#     func_name = (out_dir + '%s_ses-00_task-localizer_run-0%i_space-T1w_desc-preproc_bold_trim%d_norm.nii.gz' % (sub, run, n_trunc))\n",
    "#     image.load_img(func_name)\n",
    "#     first_vol = image.index_img(func_name,vol_num)\n",
    "#     #mean_func = mean_img(func_name)\n",
    "#     plot_epi(first_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Plot a voxel time-series <a id=\"plot_voxel\"></a>\n",
    "\n",
    "After masking, the fMRI dataset at this stage (found in the variable epi_mask_data that was created in the cell above) is in the format rows=time (i.e. 310 rows referring to 310 TRs) and columns=voxels (i.e. the number of voxels in the mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot value of voxel_id through time point 650 (there are only 618 time points)\n",
    "voxel_id = 200\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "ax.plot(epi_mask_data_postfaces[:, voxel_id])\n",
    "ax.set_title('Voxel time series, POSTFACES, voxel id = %d' % voxel_id)\n",
    "ax.set_xlabel('TR')\n",
    "ax.set_ylabel('Voxel Intensity')\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "ax.plot(epi_mask_data_familiarization[:, voxel_id])\n",
    "ax.set_title('Voxel time series, FAMILIARIZATION, voxel id = %d' % voxel_id)\n",
    "ax.set_xlabel('TR')\n",
    "ax.set_ylabel('Voxel Intensity')\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "ax.plot(epi_mask_data_reward[:, voxel_id])\n",
    "ax.set_title('Voxel time series, REWARD, voxel id = %d' % voxel_id)\n",
    "ax.set_xlabel('TR')\n",
    "ax.set_ylabel('Voxel Intensity')\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize=(14,5))\n",
    "ax.plot(epi_mask_data_decision[:, voxel_id])\n",
    "ax.set_title('Voxel time series, DECISION, voxel id = %d' % voxel_id)\n",
    "ax.set_xlabel('TR')\n",
    "ax.set_ylabel('Voxel Intensity')\n",
    "\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(epi_mask_data_all[200:300,voxel_id-100:voxel_id+100])\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check mean and standard deviation of normalized data (reward phase only) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = np.mean(epi_mask_data_reward, axis=0)\n",
    "x_std = np.std(epi_mask_data_reward, axis=0, dtype=np.float64)\n",
    "\n",
    "print('the mean of 1st few time points:\\n', x_mean[0:10])\n",
    "print('')\n",
    "print('the std of 1st few time points:\\n', x_std[0:10])\n",
    "print('')\n",
    "print(np.shape(x_mean))\n",
    "print(np.shape(x_std))\n",
    "print('')\n",
    "print(np.amin(x_mean), np.amax(x_mean))\n",
    "print(np.amin(x_std), np.amax(x_std))\n",
    "\n",
    "# print(x_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize = (14,4))\n",
    "\n",
    "n_bins = 20\n",
    "\n",
    "axes[0].hist(x_mean, bins = n_bins)\n",
    "axes[0].set_title('distribution of means')\n",
    "axes[0].set_xlabel('mean values')\n",
    "axes[0].set_ylabel('counts')\n",
    "axes[0].xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "\n",
    "axes[1].hist(x_std, bins = n_bins)\n",
    "axes[1].set_title('distribution of stds')\n",
    "axes[1].set_xlabel('std values')\n",
    "axes[1].set_ylabel('counts')\n",
    "axes[1].xaxis.set_major_locator(plt.MaxNLocator(3))\n",
    "axes[1].get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
